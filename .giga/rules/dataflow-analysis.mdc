---
description: Document processing pipeline, evidence extraction, AI analysis integration, and report generation workflow specification.
---

# === USER INSTRUCTIONS ===
description: Analyzes data flow patterns in deal analysis systems and document processing pipelines
Primary Data Flow Components:
1. Document Processing Pipeline
- Input: Raw deal documents (PDF, PPTX, DOCX)
- Processing stages:
  * Document classification and grouping
  * Content extraction and structuring
  * Evidence detection and validation
  * Quality scoring and verification
- Output: Structured deal evidence with confidence scores
2. Evidence Collection Flow
- Structured fact gathering from documents
- Multi-source verification system
- Progressive confidence scoring
- Citation tracking and validation
- Evidence completeness assessment
3. Analysis Integration Flow
- Deal data aggregation from multiple sources
- AI model integration points for analysis
- Human-in-the-loop validation gates 
- Progressive depth assessment tracking
- Analysis state management through cycles
4. Report Generation Pipeline
- Evidence collection and validation
- Confidence score aggregation
- Risk assessment compilation
- Recommendation synthesis
- Template-based report assembly
Key Data Transformations:
1. Document → Evidence
- Document classification
- Content extraction
- Fact detection
- Citation mapping
- Confidence scoring
2. Evidence → Analysis
- Fact aggregation
- Pattern detection
- Risk assessment
- Depth calculation
- Cycle progression
3. Analysis → Reports
- Finding synthesis
- Score compilation
- Risk mapping
- Recommendation generation
Critical Data Points:
- Document confidence scores
- Evidence validation status
- Analysis depth metrics
- Risk assessment values
- Recommendation confidence

description: For analyzing data flow patterns in investment deal analysis systems, focusing on document processing pipelines and report generation workflows.
The system implements a multi-stage data flow for investment deal analysis:
- Input: Raw documents (pitch decks, financials, legal docs)
  * Text extraction with OCR fallback
  * Intelligent categorization by document type
  * Metric/data point extraction
  * Evidence collection and validation
- Output: Normalized document data with confidence scores
2. Evidence Extraction Flow
- Input: Processed documents
- Extraction workflow:
  * Fact identification and extraction
  * Source attribution tracking
  * Confidence scoring (0-100)
  * Cross-validation against multiple sources
- Output: Verified evidence objects
3. AI Analysis Integration
- Input: Evidence objects
- Analysis stages:
  * Market opportunity scoring
  * Team assessment
  * Financial health calculation
  * Risk severity mapping
- Output: Analysis results with confidence bands
- Input: Analysis results and evidence
- Generation flow:
  * Section template selection
  * Evidence mapping to sections
  * Dynamic content generation
  * Risk assessment incorporation
- Output: Structured investment reports
5. Data Quality Control Flow
- Confidence thresholds at each stage (70%+ required)
- Remediation workflows for low-confidence data
- Version tracking for processed documents
- Evidence validation state management
The system maintains strict data quality controls through:
- Multi-stage validation gates
- Confidence scoring at each step
- Source attribution tracking
- Version control for processed data
Data flows through specialized processors:
- Document classification engine
- Evidence extraction system
- Analysis orchestration service
- Report compilation engine
- Quality control framework
# === END USER INSTRUCTIONS ===

# dataflow-analysis

Document Processing Pipeline:
1. Initial Document Intake
- Multi-stage document classification system
- Industry-specific document type detection
- Automatic company/deal matching algorithms
- Confidence scoring for document relevance
Path: apps/web/src/components/documents/DocumentBatchUploadModal.tsx

2. Evidence Extraction System
- Structured data extraction with confidence bands
- Financial metric detection and normalization
- Team/market/product evidence classification
- Evidence quality assessment framework
Path: apps/web/src/components/evidence/EvidencePanel.tsx

3. AI Analysis Integration
- Three-cycle analysis workflow:
  * Cycle 1: Broad scan and initial assessment
  * Cycle 2: Deep dive analysis
  * Cycle 3: Synthesis and recommendations
- Progressive depth tracking with confidence thresholds
Path: packages/core/src/services/cycle-analyzer.ts

4. Report Generation Workflow
- Evidence-backed sectional reporting
- Investment recommendation classification
- Multi-dimensional scoring aggregation
- Risk assessment matrices
Path: apps/web/src/components/reports/ProfessionalReportGenerator.tsx

Data Flow Integration Points:
- Document intelligence pipeline feeds evidence extraction
- Evidence system provides inputs for AI analysis cycles
- Analysis results flow into report generation
- Confidence scores and validation states control progression

Key Business Components:
1. Deal Intelligence Object (DIO)
- Versioned deal analysis container
- Parent-child relationship tracking
- Evidence mapping system
Path: packages/core/src/types/analysis.ts

2. Hierarchical Reasoning Model
- Multi-cycle fact extraction framework
- Uncertainty resolution workflow
- Evidence validation system
Path: packages/core/src/types/hrmdd.ts

Importance Scores:
- Document Processing: 85/100
- Evidence Extraction: 90/100 
- AI Analysis: 85/100
- Report Generation: 80/100

$END$

 If you're using this file in context, clearly say in italics in one small line that "Context added by Giga dataflow-analysis" along with specifying exactly what information was used from this file in a human-friendly way, instead of using kebab-case use normal sentence case.